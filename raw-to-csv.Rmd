---
title: 'Group Project Part I: Data Wrangling'
author: "Wranglers"
date: "Wednesday, October 7, 2015"
output: word_document
---

## Team Members

- Nisha Balani  
- Chanakya Daparthy  
- Sri Dakshinya Gudipati  
- Sai Aditya Immaneni  
- Anusha Vanam  
- Keith Williams  

# Data Preparation

The data for patent bibliographic data exists in 533 zipped files. The format for the relevant data is pseudo-xml. The following process is used to extract and clean the data:  

#### 1. Unzip and Transform Raw `ipgbXXXXXXXX_wkXX.xml` Files  

Each `*.zip` file is iteratively extracted, and, using regular expressions, the `.xml` transformed, so that each patent is on a single line. These patent per line files are filtered, so that only patents belonging to our company and its competitors are kept. See *Figure 1* for the shell code.   


#### 2. Prepare Each Company's Patents for Feature Extraction  

The one patent per line text file is split into 4 files: one for the patents belonging to our company, and three for each of our closest competitors. See *Figure 2* for the shell code.  

#### 3. Use Regex to Extract Fields and Write .csv

Each of the four one patent per line patent files are processed using a Python script (*Figure 3*). The resulting file is a .csv, where each row is a single patent and each column is a feature of that patent. The features are:  
- Company Name  
- Patent Assignee  
- Year Granted  
- Year Applied  
- Patent Class  
- Patent Number  
- Patent Title  
- Patent Abstract

*Figures 4 & 5* visualize word frequencies found in patent titles and patent abstracts for our company, Medtronic.

# Patent Ratios

The USPTO website was used to query the number of patents per year over the last 10 years for Medtronic and its three closest competitors: Stryker, Boston Scientific, and Abbott. These numbers were compared to the number of patents for each company mined from the patent bibliographic data (*Figures 6, 7, & 8*). Nearly 100% of the reported patents were mined from the bibliographic data.

```{r, echo=FALSE, message=FALSE}
# load libraries
library(dplyr)
library(ggplot2)
library(tidyr)
library(knitr)

# define uspto patent counts
med_actual <- c(257, 306, 317, 214 + 18, 286 + 46, 525 + 74, 
                522 + 49, 581 + 117, 668 + 808, 723 + 700, 151 + 216)
stryker_actual <- c(33, 30, 27, 31, 48, 65, 82, 94, 116, 117, 21)
bs_actual <- c(86, 146, 156, 192, 274, 474, 507, 477, 504, 561, 100)
abbott_actual <- c(54, 101, 85, 101, 115, 253, 433, 505, 508, 449, 76)

# load mined .csv
setwd('~/DSBA-6100/DSBA6100-GrpProject/')
patent_file <- 'medtronic_and_competitor_patents_05-15.csv'
patents <- read.csv(patent_file, stringsAsFactors=FALSE)

# count mined patents by company by year
patent_counts <- patents %>% 
    group_by(company_name, year_granted) %>% 
    summarise(mined = n()) 
patent_counts$uspto <- c(abbott_actual, bs_actual, med_actual, stryker_actual)

# compare mined patent counts to reported USPTO patent counts
patent_ratios <- gather(patent_counts, "mined", "uspto", 3:4)
names(patent_ratios)[3:4] <- c("source_found","count")
```

# Other Relevant Data

- Compare patents utilized vs. non utilized (utilization = products implemented)  
- Connect patent(s) with products and products to revenue data  
    - Product line information  
- Revenue information by product (SEC Filings 10K/10Q)  
- Macro economic scenario for medical industry  
- Patent classification is alpha numeric- dictionary of these codes and translate them to business information  
- Medical eco system: Wearable technology (FitBit, Jawbone, Google Lens etc.)

# Appendix

#### Figure 1 

Shell script for extracting, transforming, and filtering patent .xml files.

```{r eval=FALSE}
shopt -s extglob #set shell options

cat >> oppl.txt #create empty text file for appending  
^D

# loop over each zip, extract and transform
for file in *.zip
do
unzip "$file"
cat *.xml | python ../one-patent-per-line.py | grep -i -E "medtronic|covidien|stryker|boston scientfic|abbott" >> ../oppl.txt
rm *.!(zip)
done
```

#### Figure 2

Shell script for splitting one patent per line file into 4 files, one for each company.

```{r eval=FALSE}
grep -i -E "<assignee>.*?medtronic.*?</orgname|<assignee>.*?covidien.*?</orgname" oppl.txt > oppl_medtronic.txt
grep -i -E "<assignee>.*?stryker.*?</orgname" oppl.txt > oppl_stryker.txt
grep -i -E "<assignee>.*?boston scientific.*?</orgname" oppl_combined.txt > oppl_boston_scientific.txt
grep -i -E "<assignee>.*?abbott.*?</orgname" oppl_combined.txt > oppl_abbott.txt
```

#### Figure 3

Python script for creating structured .csv.

```{r eval=FALSE}
# Takes as an input a list of .txt files in which each patent is on its own 
# line: an output of one-patent-per-line.py | grep -i "COMPANY" > out.txt
# Extracts fields for output as a tidy .csv

import sys as sys    # for unix pipes
import re as re      # for regular expressions
import pandas as pd  # for structured data tables

def oppl_to_df(company, patent_file):
    """Takes a wide patent file, and returns a pandas DataFrame
    Inputs:
        company: string of company name
        patent_file: .txt file where each patent owned by company is on a single line
    Ouput:
        pandas DataFrame, where rows are patents, and columns are the features:
            company name
            patent assignee
            year granted
            year applied
            patent class
            patent number
            patent title
            patent abstract
    """
    
    # initialize fields as a dictionary
    fields = {'company_name' : [],
              'patent_assignee' : [],
              'year_granted' : [],
              'year_applied' : [],
              'patent_class' : [],
              'patent_number' : [],
              'patent_title' : [],
              'patent_abstract' : []}
    
    # specify regular expressions
    assignee_re = '<assignee>.*?</orgname>'
    pub_ref_re = '<publication-reference>.*?</publication-reference>'
    date_tags = '<publication-reference>.*<date>|</date>.*'
    id_tags = '<publication-reference>.*<doc-number>|</doc-number>.*'
    app_ref_re = '<application-reference.*?</application-reference>'
    class_re = 'main-classification.*?<country>US.*?<main-classification>.*?</main-classification>'
    title_re = '<invention-title.*?</invention-title>'
    abstract_re = '<abstract.*?</abstract>'
  
    for line in patent_file.readlines():
        # Use regex to find fields, 
        # and append them to appropriate dictionary value
        
        # attach company name
        fields['company_name'].append(company)
        
        # extract patent assignee
        assignee = re.search(assignee_re, line)
        if assignee:
            patent_assignee = re.sub('<assignee>.*<orgname>|</orgname>', 
                                  '', 
                                  assignee.group())
            fields['patent_assignee'].append(patent_assignee)
        else:
            fields['patent_assignee'].append('None')
            
        # extract year granted and patent number, 
        # both in <publication-reference>
        pub_ref = re.search(pub_ref_re, line)
        if pub_ref:
            year_granted = re.sub(date_tags, '', pub_ref.group())
            patent_number = re.sub(id_tags, '', pub_ref.group())
            fields['year_granted'].append(year_granted[0:4])
            fields['patent_number'].append(patent_number)
        else:
            fields['year_granted'].append('None')
            fields['patent_number'].append('None')
            
        # extract year applied
        app_ref = re.search(app_ref_re, line)
        if app_ref:
            year_applied = re.sub('<application.*<date>|</date>.*',
                                  '',
                                  app_ref.group())
            fields['year_applied'].append(year_applied[0:4])
        else:
            fields['year_applied'].append('None')
        
        # extract patent classification
        classification = re.search(class_re, line)
        if classification:
            patent_class = re.sub('.*<main-classification>|</main.*',
                                  '',
                                  classification.group())
            fields['patent_class'].append(patent_class)
        else:
            fields['patent_class'].append('None')
                
        # extract patent title
        title = re.search(title_re, line)
        if title:
            patent_title = re.sub('<invention.*?>|</invention.*',
                                  '',
                                  title.group())
            fields['patent_title'].append(patent_title)
        else:
            fields['patent_title'].append('None')
        
        # extract patent abstract
        abstract_field = re.search(abstract_re, line)
        if abstract_field:
            abstract = re.sub('<abstract.*<p.*?>|</abstract>| <endline> ',
                              '',
                              abstract_field.group())
            fields['patent_abstract'].append(abstract)
        else:
            fields['patent_abstract'].append('None')
            
    # transform dictionary to pandas DataFrame
    return pd.DataFrame(fields)
        
def run():
    # paths to patent files for each company. Each patent is on a single line.
    oppl_medtronic = open('../oppl_medtronic.txt')
    oppl_stryker = open('../oppl_stryker.txt')
    oppl_bs = open('../oppl_boston_scientific.txt')
    oppl_abbott = open('../oppl_abbott.txt')
    
    # define lists of companies, and the patent files for feature extraction loop
    oppl_files = [oppl_medtronic, oppl_stryker, oppl_bs, oppl_abbott]
    companies = ['Medtronic', 'Stryker', 'Boston Scientific', 'Abbott']
    
    # convert each patent file into a tidy pandas data frame
    dfs = [oppl_to_df(companies[i], oppl_files[i]) for i in range(len(companies))]
    
    # concatenate the data frames, and write to .csv
    df_combined = pd.concat(dfs, ignore_index=True)
    df_combined.to_csv('medtronic_and_competitor_patents_05-15.csv', index=False)

if __name__ == "__main__":
    run()
```

#### Figure 4

Visualization of word freqencies in Medtronic patent titles. R code provided.

```{r, message=FALSE, warning=FALSE}
library(wordcloud)   # Package for word clouds
library(wesanderson) # color palettes

# filter company equal to Medtronic
med <- patents %>% filter(company_name == 'Medtronic')

# visualize patent titles
palette <- wes_palette("Darjeeling", 5, "discrete")
palette <- palette[5:1]
wordcloud(med$patent_title, max.words=150, colors=palette)
```

#### Figure 5

Visualization of word frequencies in Medtronic patent abstracts. R code provied.

```{r}
# visualize patent abstracts
wordcloud(med$patent_abstract, max.words=150, colors=palette)
```

#### Figure 6

By year comparison of mined patents to patents reported on the USPTO website. R code for creating visualization provided.

```{r, fig.height=5, fig.width=8}
# visualize ratios by year
plot_ratios <- ggplot(patent_ratios, aes(company_name, count, fill=source_found)) +
    geom_bar(stat='identity', position='dodge') +
    facet_grid(.~year_granted, scales="free_y") +
    labs(list(title = "Patent Counts: Biblio Data vs USPTO",
              x = "Company Name",
              y = "Number of Patents")) +
    theme(axis.text.x = element_text(angle=90, hjust=1, vjust=0.5))
plot_ratios
```

#### Figure 7

Comparison of total mined patents to patents reported on the USPTO website from 2005 to March 2015. R code for creating visualization provided.

```{r, fig.width=8}
# visualize ratios: total by company
plot_totals <- ggplot(patent_ratios, aes(company_name, count, fill=source_found)) +
    geom_bar(stat='identity', position='dodge') +
    labs(list(title = "Patent Counts: Biblio Data vs USPTO",
              x = "Company Name",
              y = "Number of Patents")) +
    theme(title = element_text(size = 16))
plot_totals
```

#### Figure 8

Table of mined *vs.* reported patents.

```{r, echo=FALSE}
kable(patent_counts, align='c')
```